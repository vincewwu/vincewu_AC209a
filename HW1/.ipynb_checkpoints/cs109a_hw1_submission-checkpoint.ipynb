{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 1\n",
    "\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "**WARNING**: There is web page scraping in this homework. It takes about 40 minutes. **Do not wait till the last minute** to do this homework.\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. There is an important CAVEAT to this. DO NOT run the web-page fetching cells again. (We have provided hints like `# DO NOT RERUN THIS CELL WHEN SUBMITTING` on some of the cells where we provide the code). Instead load your data structures from the JSON files we will ask you to save below. Otherwise you will be waiting for a long time. (Another reason to not wait until the last moment to submit.)\n",
    "\n",
    "- Do not include your name in the notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Rihanna or Mariah?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Billboard Magazine puts out a top 100 list of \"singles\" every week. Information from this list, as well as that from music sales, radio, and other sources is used to determine a top-100 \"singles\" of the year list. A **single** is typically one song, but sometimes can be two songs which are on one \"single\" record.\n",
    "\n",
    "In this homework you will: \n",
    "\n",
    "1. Scrape Wikipedia to obtain infprmation about the best singers and groups from each year (distinguishing between the two groups) as determined by the Billboard top 100 charts. You will have to clean this data. Along the way you will learn how to save data in json files to avoid repeated scraping. \n",
    "2. Scrape Wikipedia to obtain information on these singers. You will have to scrape the web pages, this time using a cache to guard against network timeouts (or your laptop going to sleep). You will again clean the data, and save it to a json file.\n",
    "3. Use pandas to represent these two datasets and merge them.\n",
    "4. Use the individual and merged datasets to visualize the performance of the artists and their songs. We have kept the amount of analysis limited here for reasons of time; but you might enjoy exploring music genres and other aspects of the music business you can find on these wikipedia pages at your own leisure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have worked through Lab0 and Lab 1, and Lecture 2.  Lab 2 will help as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, first we import the necessary libraries.  In particular, we use [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/) to give us a nicer default color palette, with our plots being of large (`poster`) size and with a white-grid background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Scraping Wikipedia for Billboard Top 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question you will scrape Wikipedia for the Billboard's top 100 singles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Wikipedia for Billboard singles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using  [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/), and suggest that you use Python's built in `requests` library to fetch the web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Parsing the Billboard Wikipedia page for 1970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the web page at http://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1970 using a HTTP GET request. From this web page we'll extract the top 100 singles and their rankings. Create a list of dictionaries, 100 of them to be precise, with entries like \n",
    "\n",
    "`{'url': '/wiki/Sugarloaf_(band)', 'ranking': 30, 'band_singer': 'Sugarloaf', 'title': 'Green-Eyed Lady'}`. \n",
    "\n",
    "If you look at that web page, you'll see a link for every song, from which you can get the `url` of the singer or band. We will use these links later to scrape information about the singer or band. From the listing we can also get the band or singer name `band_singer`, and `title` of the song.\n",
    "\n",
    "*HINT: look for a table with class `wikitable`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "You should get something similar to this (where songs is the aforementioned list):\n",
    "\n",
    "```\n",
    "songs[2:4]\n",
    "```\n",
    "\n",
    "```\n",
    "[{'band_singer': 'The Guess Who',\n",
    "  'ranking': 3,\n",
    "  'title': '\"American Woman\"',\n",
    "  'url': '/wiki/The_Guess_Who'},\n",
    " {'band_singer': 'B.J. Thomas',\n",
    "  'ranking': 4,\n",
    "  'title': '\"Raindrops Keep Fallin\\' on My Head\"',\n",
    "  'url': '/wiki/B.J._Thomas'}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the wikipedia page for 1970\n",
    "req_1970 = requests.get(\"http://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1970\")\n",
    "page = req_1970.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup_1970 = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the correct table\n",
    "# Since it is guaranteed to be uinique, use find rather than find_all\n",
    "table_1970 = soup_1970.find(\"table\", \"wikitable\", \"sortable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract all the rows\n",
    "rows_1970 = [row for row in table_1970.find_all(\"tr\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [\"ranking\", \"title\", \"band_singer\", \"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values_1970 = []\n",
    "for row in rows_1970[1:]:\n",
    "    value_row = []\n",
    "    \n",
    "    # Getting all the values\n",
    "    for v in row.find_all(\"td\"):\n",
    "        value_row.append(v.get_text())\n",
    "    value_row[0] = int(value_row[0])\n",
    "    \n",
    "    # Getting the singer url- the last url is the singer url\n",
    "    urls = row.find_all(\"a\")\n",
    "    value_row.append(urls[-1]['href'])\n",
    "    \n",
    "    values_1970.append(tuple(value_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_1970 = [{col: val for col, val in zip(columns, values)} for values in values_1970]\n",
    "songs_1970[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Generalize the previous: scrape Wikipedia from 1992 to 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visiting the urls similar to the ones for 1970, we can obtain the billboard top 100 for the years 1992 to 2014. (We choose these later years rather than 1970 as you might find music from this era more interesting.) Download these using Python's `requests` module and store the text from those requests in a dictionary called `yearstext`. This dictionary ought to have as its keys the years (as integers from 1992 to 2014), and as values corresponding to these keys the text of the page being fetched.\n",
    "\n",
    "You ought to sleep a second (look up `time.sleep` in Python) at the very least in-between fetching each web page: you do not want Wikipedia to think you are a marauding bot attempting to mount a denial-of-service attack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*HINT: you might find `range` and string-interpolation useful to construct the URLs *.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year_list = list(range(1992, 2015))\n",
    "year_list\n",
    "address_pre = \"http://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_\"\n",
    "yearstext = dict();\n",
    "for this_year in year_list:\n",
    "    this_address = address_pre + str(this_year)\n",
    "    req = requests.get(this_address)\n",
    "    page = req.text\n",
    "    yearstext[this_year] = page\n",
    "    \n",
    "    time.sleep(1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Parse and Clean data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the code you wrote to get data from 1970 which produces a list of dictionaries, one corresponding to each single.  Now write a function `parse_year(the_year, yeartext_dict)` which takes the year, prints it out, gets the text for the year from the just created `yearstext` dictionary, and return a list of dictionaries for that year, with one dictionary for each single. Store this list in the variable `yearinfo`.\n",
    "\n",
    "The dictionaries **must** be of this form:\n",
    "\n",
    "```\n",
    "{'band_singer': ['Brandy', 'Monica'],\n",
    "  'ranking': 2,\n",
    "  'song': ['The Boy Is Mine'],\n",
    "  'songurl': ['/wiki/The_Boy_Is_Mine_(song)'],\n",
    "  'titletext': '\" The Boy Is Mine \"',\n",
    "  'url': ['/wiki/Brandy_Norwood', '/wiki/Monica_(entertainer)']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spec of this function is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "parse_year\n",
    "\n",
    "Inputs\n",
    "------\n",
    "the_year: the year you want the singles for\n",
    "yeartext_dict: a dictionary with keys as integer years and values the downloaded web pages \n",
    "    from wikipedia for that year.\n",
    "   \n",
    "Returns\n",
    "-------\n",
    "\n",
    "a list of dictionaries, each of which corresponds to a single and has the\n",
    "following data:\n",
    "\n",
    "Eg:\n",
    "\n",
    "{'band_singer': ['Brandy', 'Monica'],\n",
    "  'ranking': 2,\n",
    "  'song': ['The Boy Is Mine'],\n",
    "  'songurl': ['/wiki/The_Boy_Is_Mine_(song)'],\n",
    "  'titletext': '\" The Boy Is Mine \"',\n",
    "  'url': ['/wiki/Brandy_Norwood', '/wiki/Monica_(entertainer)']}\n",
    "  \n",
    "A dictionary with the following data:\n",
    "    band_singer: a list of bands/singers who made this single\n",
    "    song: a list of the titles of songs on this single\n",
    "    songurl: a list of the same size as song which has urls for the songs on the single \n",
    "        (see point 3 above)\n",
    "    ranking: ranking of the single\n",
    "    titletext: the contents of the table cell\n",
    "    band_singer: a list of bands or singers on this single\n",
    "    url: a list of wikipedia singer/band urls on this single: only put in the part \n",
    "        of the url from /wiki onwards\n",
    "    \n",
    "\n",
    "Notes\n",
    "-----\n",
    "See description and example above.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "columns = [\"ranking\", \"titletext\", \"song\", \"songurl\", \"band_singer\", \"url\"]\n",
    "\n",
    "def parse_year(the_year, yeartext_dict):\n",
    "    page = yeartext_dict[the_year]\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    table = soup.find(\"table\", \"wikitable\", \"sortable\")\n",
    "    rows = [row for row in table.find_all(\"tr\")]\n",
    "    info_all = []\n",
    "    for row in rows[1:]:\n",
    "        info_row = []\n",
    "        # Check if the rank is in <td> or <th>\n",
    "        info_th = row.find(\"th\")\n",
    "        info_td = row.find_all(\"td\")\n",
    "        if info_th:\n",
    "            rank = int(info_th.get_text())\n",
    "            ind = 0;\n",
    "        else:\n",
    "            rank = int(info_td[0].get_text()) # First item is ranking\n",
    "            ind = 1;\n",
    "            print(\"Ranking in td %d %d\" % (the_year, rank))\n",
    "        info_row.append(rank) # Append ranking\n",
    "        \n",
    "        # Next are song's title and url\n",
    "        titletext = info_td[ind].get_text()\n",
    "        info_row.append(titletext) # Append titletext\n",
    "        titletext_splt = titletext.split(\" / \")\n",
    "        title_w_url = info_td[ind].find_all(\"a\")\n",
    "        song = []\n",
    "        songurl = []\n",
    "        # Some songs are partially linked with url, so splitted title gives the most complete song name(s)\n",
    "        # See Rank #1 song in 1996 Billboard\n",
    "        for t in titletext_splt:\n",
    "            if (t != '' and t != ' '): # Get rid of empty name\n",
    "                song.append(t.replace('\"', ''))\n",
    "                if title_w_url:\n",
    "                    for v in title_w_url:\n",
    "                        t_partial = v.get_text()\n",
    "                        if (t_partial in t) or (t in t_partial):\n",
    "                            songurl.append(v['href'])\n",
    "                            break\n",
    "                else:\n",
    "                    songurl.append(None)\n",
    "        info_row.append(song) # Append song\n",
    "        info_row.append(songurl) # Append song url\n",
    "        \n",
    "        # Next are singer and url\n",
    "        ind += 1\n",
    "        singer_text = info_td[ind].get_text()\n",
    "        singer_splt = re.split(\" featuring | and |, \", singer_text)\n",
    "        singer_w_url = info_td[ind].find_all(\"a\")\n",
    "        band_singer = []\n",
    "        url = []\n",
    "        # Some singers' name may be partially linked with url too (not in 1992-2014 though)\n",
    "        # Just to be general here\n",
    "        for s in singer_splt:\n",
    "            if (s != '' and s != ' '): # Get rid of empty name\n",
    "                band_singer.append(s)\n",
    "                if singer_w_url:\n",
    "                    for v in singer_w_url:\n",
    "                        s_partial = v.get_text()\n",
    "                        if (s_partial in s) or (s in s_partial):\n",
    "                            url.append(v['href'])\n",
    "                            break\n",
    "                else:\n",
    "                    url.append(None)\n",
    "        info_row.append(band_singer) # Append band_singer\n",
    "        info_row.append(url) # Append singer url\n",
    "        # Add everything into info_all\n",
    "        info_all.append(tuple(info_row))\n",
    "        \n",
    "    yearinfo = [{col: val for col, val in zip(columns, info)} for info in info_all]\n",
    "    return(yearinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parse_year(1997, yearstext)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary with key = year and value = the list of dictionary for that year\n",
    "yearinfo = dict()\n",
    "for y in range(1992, 2015):\n",
    "    this_yearinfo = parse_year(y, yearstext)\n",
    "    yearinfo[y] = this_yearinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful notes\n",
    "\n",
    "Notice that some singles might have multiple songs:\n",
    "\n",
    "```\n",
    "{'band_singer': ['Jewel'],\n",
    "  'ranking': 2,\n",
    "  'song': ['Foolish Games', 'You Were Meant for Me'],\n",
    "  'songurl': ['/wiki/Foolish_Games',\n",
    "   '/wiki/You_Were_Meant_for_Me_(Jewel_song)'],\n",
    "  'titletext': '\" Foolish Games \" / \" You Were Meant for Me \"',\n",
    "  'url': ['/wiki/Jewel_(singer)']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some singles don't have a song URL:\n",
    "\n",
    "```\n",
    "{'band_singer': [u'Nu Flavor'],\n",
    "  'ranking': 91,\n",
    "  'song': [u'Heaven'],\n",
    "  'songurl': [None],\n",
    "  'titletext': u'\"Heaven\"',\n",
    "  'url': [u'/wiki/Nu_Flavor']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus there are some issues this function must handle:\n",
    "\n",
    "1. There can be more than one  `band_singer` as can be seen above (sometimes with a comma, sometimes with \"featuring\" in between). The best way to parse these is to look for the urls.\n",
    "2. There can be two songs in a single, because of the way the industry works: there are two-sided singles. See https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1997 for an example. You can find other examples in 1998 and 1999.\n",
    "3. The `titletext` is the contents of the table cell, and retains the quotes that Wikipedia puts on the single.\n",
    "4. If no song anchor is found (see the 24th song in the above url), assume there is one song in the single, set `songurl` to [`None`] and the song name to the contents of the table cell with the quotes stripped (ie `song` is a one-element list with this the `titletext` stripped of its quotes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, we can do this for 1997. We'll print the first 5 outputs: `parse_year(1997, yearstext)[:5]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give the following. Notice that the year 1997 exercises the edge cases we talked about earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[{'band_singer': ['Elton John'],\n",
    "  'ranking': 1,\n",
    "  'song': ['Something About the Way You Look Tonight',\n",
    "   'Candle in the Wind 1997'],\n",
    "  'songurl': ['/wiki/Something_About_the_Way_You_Look_Tonight',\n",
    "   '/wiki/Candle_in_the_Wind_1997'],\n",
    "  'titletext': '\" Something About the Way You Look Tonight \" / \" Candle in the Wind 1997 \"',\n",
    "  'url': ['/wiki/Elton_John']},\n",
    " {'band_singer': ['Jewel'],\n",
    "  'ranking': 2,\n",
    "  'song': ['Foolish Games', 'You Were Meant for Me'],\n",
    "  'songurl': ['/wiki/Foolish_Games',\n",
    "   '/wiki/You_Were_Meant_for_Me_(Jewel_song)'],\n",
    "  'titletext': '\" Foolish Games \" / \" You Were Meant for Me \"',\n",
    "  'url': ['/wiki/Jewel_(singer)']},\n",
    " {'band_singer': ['Puff Daddy', 'Faith Evans', '112'],\n",
    "  'ranking': 3,\n",
    "  'song': [\"I'll Be Missing You\"],\n",
    "  'songurl': ['/wiki/I%27ll_Be_Missing_You'],\n",
    "  'titletext': '\" I\\'ll Be Missing You \"',\n",
    "  'url': ['/wiki/Sean_Combs', '/wiki/Faith_Evans', '/wiki/112_(band)']},\n",
    " {'band_singer': ['Toni Braxton'],\n",
    "  'ranking': 4,\n",
    "  'song': ['Un-Break My Heart'],\n",
    "  'songurl': ['/wiki/Un-Break_My_Heart'],\n",
    "  'titletext': '\" Un-Break My Heart \"',\n",
    "  'url': ['/wiki/Toni_Braxton']},\n",
    " {'band_singer': ['Puff Daddy', 'Mase'],\n",
    "  'ranking': 5,\n",
    "  'song': [\"Can't Nobody Hold Me Down\"],\n",
    "  'songurl': ['/wiki/Can%27t_Nobody_Hold_Me_Down'],\n",
    "  'titletext': '\" Can\\'t Nobody Hold Me Down \"',\n",
    "  'url': ['/wiki/Sean_Combs', '/wiki/Mase']}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a json file of information from the scraped files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to lose all this work, so let's save the last data structure we created to disk. That way if you need to re-run from here, you don't need to redo all these requests and parsing. \n",
    "\n",
    "DO NOT RERUN THE HTTP REQUESTS TO WIKIPEDIA WHEN SUBMITTING.\n",
    "\n",
    "*We **DO NOT** need to see these JSON files in your submission!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DO NOT RERUN THIS CELL WHEN SUBMITTING\n",
    "fd = open(\"data/yearinfo.json\",\"w\")\n",
    "json.dump(yearinfo, fd)\n",
    "fd.close()\n",
    "del yearinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reload our JSON file into the yearinfo variable, just to be sure everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RERUN WHEN SUBMITTING\n",
    "# Another way to deal with files. Has the advantage of closing the file for you.\n",
    "with open(\"data/yearinfo.json\", \"r\") as fd:\n",
    "    yearinfo = json.load(fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Construct a year-song-singer dataframe from the yearly information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct a dataframe `flatframe` from the `yearinfo`. The frame should be similar to the frame below.  Each row of the frame represents a song, and carries with it the chief properties of year, song, singer, and ranking.\n",
    "\n",
    "![](https://raw.githubusercontent.com/cs109/a-2017/master/hwassets/images/HW1SC1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the dataframe, we'll need to iterate over the years and the singles per year. Notice how, above, the dataframe is ordered by ranking and then year. While the exact order is up to you, note that you will have to come up with a scheme to order the information.\n",
    "\n",
    "Check that the dataframe has sensible data types. You will also likely find that the year field has become an \"object\" (Pandas treats strings as generic objects): this is due to the conversion to and back from JSON. Such conversions need special care. Fix any data type issues with `flatframe`. (See Pandas [astype](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.astype.html) function.) \n",
    "We will use this `flatframe` in the next question. \n",
    "\n",
    "(As an aside, we used the name `flatframe` to indicate that this dataframe is flattened from a hierarchical dictionary structure with the keys being the years.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame\n",
    "flatframe = pd.DataFrame()\n",
    "joinlist = ['band_singer', 'song', 'songurl', 'url']\n",
    "for y in range(1992, 2015):\n",
    "    yearframe = pd.DataFrame.from_records(yearinfo[str(y)])\n",
    "    yearframe['year'] = y\n",
    "    del yearframe['titletext']\n",
    "    # Join the list of various lenth into one string before storing in the DataFrame\n",
    "    # This makes data analysis easilier (otherwise can't use function like value_counts())\n",
    "    yearframe[joinlist] = yearframe[joinlist].applymap(lambda l: ', '.join(filter(None, l)))\n",
    "    flatframe = flatframe.append(yearframe)\n",
    "flatframe.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearange the DataFrame columns and re-index based on sorting\n",
    "flatframe = flatframe[['year', 'band_singer', 'ranking', 'song', 'songurl', 'url']]\n",
    "flatframe = flatframe.sort_values(by=['ranking', 'year'])\n",
    "flatframe.index = range(len(flatframe))\n",
    "flatframe.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who are the highest quality singers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple singers, find the first singer (main).\n",
    "# This makes the counts more accurate\n",
    "flatframe['first_singer'] = flatframe['band_singer'].apply(lambda n: n.split(', ')[0])\n",
    "flatframe.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show the highest quality singers and plot them on a bar chart.\n",
    "\n",
    "#### 1.5 Find highest quality singers according to how prolific they are\n",
    "\n",
    "What do we mean by highest quality? This is of course open to interpretation, but let's define \"highest quality\" here as the number of times a singer appears in the top 100 over this time period. If a singer appears twice in a year (for different songs), this is counted as two appearances, not one. \n",
    "\n",
    "Make a bar-plot of the most prolific singers. Singers on this chart should have appeared at-least more than 15 times. (HINT: look at the docs for the pandas method `value_counts`.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_defaults()\n",
    "sns.set_context(\"notebook\")\n",
    "singer_appear_count = flatframe['first_singer'].value_counts()\n",
    "singer_appear_count = singer_appear_count[singer_appear_count >= 15].sort_values()\n",
    "count = singer_appear_count.values\n",
    "num = np.arange(len(singer_appear_count))\n",
    "singer_appear_count.plot.barh()\n",
    "ax = plt.gca()\n",
    "plt.ylabel(\"Appearence\")\n",
    "for c, n in zip(count, num):\n",
    "    plt.annotate(str(c), xy=(c+0.1, n), va='center')\n",
    "xt = np.arange(0, count.max(), 5)\n",
    "plt.xticks(xt, [''] * len(xt))\n",
    "plt.grid(axis='x', color='white', linestyle='-')\n",
    "ax.tick_params(axis='both', which='both', length=0)\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 What if we used a different metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we would like to capture is this: a singer should to be scored higher if the singer appears higher in the rankings. So we'd say that a singer who appeared once at a higher and once at a lower ranking is a \"higher quality\" singer than one who appeared twice at a lower ranking. \n",
    "\n",
    "To do this, group all of a singers songs together and assign each song a score `101 - ranking`. Order the singers by their total score and make a bar chart for the top 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "singer_group = flatframe.groupby('first_singer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_dict = dict()\n",
    "for name, group in singer_group:\n",
    "    score_dict[name] = 101 * len(group) - group['ranking'].agg(np.sum)\n",
    "scoreframe = pd.DataFrame.from_dict(score_dict, orient='index')\n",
    "scoreframe.columns = ['score']\n",
    "scoreframe = scoreframe.sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_defaults()\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "score_top20 = scoreframe[:20].sort_values(by='score')\n",
    "score = score_top20['score']\n",
    "num = np.arange(len(score_top20))\n",
    "score_top20.plot.barh()\n",
    "\n",
    "ax = plt.gca()\n",
    "plt.ylabel(\"Quality Score\")\n",
    "for s, n in zip(score, num):\n",
    "    plt.annotate(str(s), xy=(s+10, n), va='center')\n",
    "xt = np.arange(0, score.max(), 200)\n",
    "plt.xticks(xt, [''] * len(xt))\n",
    "plt.grid(axis='x', color='white', linestyle='-')\n",
    "ax.legend_.remove()\n",
    "ax.tick_params(axis='both', which='both', length=0)\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Do you notice any major differences when you change the metric?\n",
    "\n",
    "How have the singers at the top shifted places? Why do you think this happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some singers move up into the top list with less than 15 times appearence in the Billboard 100. This is simply because their songs rank on average higher (i.e. higher score) in the Billboard 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Scraping and Constructing: Information about Artists, Bands and Genres from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next job is to use those band/singer urls we collected under `flatframe.url` and get information about singers and/or bands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape information about artists from wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to fetch information about the singers or groups for all the winning songs in a list of years.\n",
    "\n",
    "Here we show a function that fetches information about a singer or group from their url on wikipedia. We create a cache object `urlcache` that will avoid redundant HTTP requests (e.g. an artist might have multiple singles on a single year, or be on the list over a span of years). Once we have fetched information about an artist, we don't need to do it again. The caching also helps if the network goes down, or the target website is having some problems. You simply need to run the `get_page` function below again, and the `urlcache` dictionary will continue to be filled.\n",
    "\n",
    "If the request gets an HTTP return code different from 200, (such as a 404 not found or 500 Internal Server Error) the cells for that URL will have a value of 1; and if the request completely fails (e.g. no network connection) the cell will have a value of 2. This will allow you to analyse the failed requests.\n",
    "\n",
    "Notice that we have wrapped the call in whats called _an exception block_. We try to make the request. If it fails entirely, or returns a HTTP code thats not 200, we set the status to 2 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urlcache={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_page(urls):\n",
    "    if not (urls is None):\n",
    "        for url in urls.split(', '):\n",
    "            print(url)\n",
    "            # Check if URL has already been visited.\n",
    "            if (url not in urlcache) or (urlcache[url]==1) or (urlcache[url]==2):\n",
    "                time.sleep(1)\n",
    "                # try/except blocks are used whenever the code could generate an exception (e.g. division by zero).\n",
    "                # In this case we don't know if the page really exists, or even if it does, if we'll be able to reach it.\n",
    "                try:\n",
    "                    r = requests.get(\"http://en.wikipedia.org%s\" % url)\n",
    "\n",
    "                    if r.status_code == 200:\n",
    "                        print(\"Success\")\n",
    "                        urlcache[url] = r.text\n",
    "                    else:\n",
    "                        print(r.status_code)\n",
    "                        urlcache[url] = 1\n",
    "                except:\n",
    "                    print(\"Fail\")\n",
    "                    urlcache[url] = 2\n",
    "        return urlcache[url]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort the `flatframe` by year, ascending, first. Think why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flatframe=flatframe.sort_values('year')\n",
    "flatframe.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pulling and saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT RERUN THIS CELL WHEN SUBMITTING\n",
    "# Here we are populating the url cache\n",
    "# subsequent calls to this cell should be very fast, since Python won't\n",
    "# need to fetch the page from the web server.\n",
    "# NOTE this function will take quite some time to run (about 30 mins for me), since we sleep 1 second before\n",
    "# making a request. If you run it again it will be almost instantaneous, save requests that might have failed\n",
    "# (you will need to run it again if requests fail..see cell below for how to test this)\n",
    "flatframe[\"url\"].apply(get_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have to run this function again and again, in case there were network problems. Note that, because there is a \"global\" cache, it will take less time each time you run it. Also note that this function is designed to be run again and again: it attempts to make sure that there are no unresolved pages remaining. Let us make sure of this: *the sum below should be 0, and the boolean True.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT RERUN THIS CELL WHEN SUBMITTING\n",
    "print(\"Number of bad requests:\",np.sum([(urlcache[k]==1) or (urlcache[k]==2) for k in urlcache])) # no one or 0's)\n",
    "print(\"Did we get all urls?\", len(flatframe.url.unique())==len(urlcache)) # we got all of the urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urlcache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the `urlcache` to disk, just in case we need it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DO NOT RERUN THIS CELL WHEN SUBMITTING\n",
    "with open(\"data/artistinfo.json\",\"w\") as fd:\n",
    "    json.dump(urlcache, fd)\n",
    "del urlcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RERUN WHEN SUBMITTING\n",
    "with open(\"data/artistinfo.json\") as json_file:\n",
    "    urlcache = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Extract information about singers and bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From each page we collected about a singer or a band, extract the following information:\n",
    "\n",
    "1. If the page has the text \"Born\" in the sidebar on the right, extract the element with the class `.bday`. If the page doesn't contain \"Born\", store `False`.  Store either of these into the variable `born`. We want to analyze the artist's age.\n",
    "\n",
    "2. If the text \"Years active\" is found, but no \"born\", assume a band. Store into the variable `ya` the value of the next table cell corresponding to this, or `False` if the text is not found.\n",
    "\n",
    "Put this all into a function `singer_band_info` which takes the singer/band url as argument and returns a dictionary `dict(url=url, born=born, ya=ya)`.\n",
    "\n",
    "The information can be found on the sidebar on each such wikipedia page, as the example here shows:\n",
    "\n",
    "![sandg](https://raw.githubusercontent.com/cs109/a-2017/master/hwassets/images/sandg.png).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the function `singer_band_info` according to the following specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "singer_band_info\n",
    "\n",
    "Inputs\n",
    "------\n",
    "url: the url\n",
    "page_text: the text associated with the url\n",
    "   \n",
    "Returns\n",
    "-------\n",
    "A dictionary with the following data:\n",
    "    url: copy the input argument url into this value\n",
    "    born: the artist's birthday\n",
    "    ya: years active variable\n",
    "\n",
    "Notes\n",
    "-----\n",
    "See description above. Also note that some of the genres urls might require a \n",
    "bit of care and special handling.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singer_band_info(url, page_text):\n",
    "    info_dict = {'url': url}\n",
    "    soup = BeautifulSoup(page_text, 'html.parser')\n",
    "    infobox = soup.find(\"table\", \"infobox\")\n",
    "    if infobox: # See B-Rock and the Bizz in Billboard 1997 rank 68 - no box\n",
    "        rows = [row for row in infobox.find_all(\"tr\")]\n",
    "        rows_text = []\n",
    "        for row in rows:\n",
    "            has_text = row.find(\"th\")\n",
    "            if has_text:\n",
    "                rows_text.append(has_text.get_text())\n",
    "            else:\n",
    "                rows_text.append('')\n",
    "        # Extract born and years active\n",
    "        if 'Born' in rows_text:\n",
    "            born_ind = rows_text.index('Born')\n",
    "            born_text = rows[born_ind].find(\"td\").get_text()\n",
    "            born_splt = born_text.split(\"(\")\n",
    "            if len(born_splt) > 1: # See Donna Lewis in Billboard 1997 Rank 64 - has born tag but no date\n",
    "                born = born_splt[1].split(\")\")[0] # Get a format of xxxx-xx-xx\n",
    "                if len(born) != 10:\n",
    "                    born = False\n",
    "            else:\n",
    "                born = False\n",
    "        else:\n",
    "            born = False\n",
    "        info_dict['born'] = born\n",
    "        if 'Years active' in rows_text:\n",
    "            ya_ind = rows_text.index('Years active')\n",
    "            ya = rows[ya_ind].find(\"td\").get_text()\n",
    "        elif 'Years\\xa0active' in rows_text:\n",
    "            ya_ind = rows_text.index('Years\\xa0active') # Some of the Years active string are different\n",
    "            ya = rows[ya_ind].find(\"td\").get_text()\n",
    "        else:\n",
    "            ya = False\n",
    "        if ya and ('\\n' in ya):\n",
    "            ya = ya.replace('\\n', '')\n",
    "        info_dict['ya'] = ya\n",
    "    else:\n",
    "        info_dict['born'] = False\n",
    "        info_dict['ya'] = False\n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if '' in urlcache:\n",
    "    del urlcache['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2  Merging this information in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the items in the singer-group dictionary cache `urlcache`, run the above function, and create a dataframe from there with columns `url`, `born`, and `ya`. Merge this dataframe on the `url` key with `flatframe`, creating a rather wide dataframe that we shall call `largedf`. It should look something like this:\n",
    "\n",
    "![](https://raw.githubusercontent.com/cs109/a-2017/master/hwassets/images/HW1SC3.png)\n",
    "\n",
    "Notice how the `born` and `ya` and `url` are repeated every time a different song from a given band is represented in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatframe['born'] = False\n",
    "flatframe['ya'] = False\n",
    "flatframe.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate a empty list to store all the info (born and ya)\n",
    "info_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = flatframe['url'].tolist()\n",
    "for urls in flatframe['url']:\n",
    "    if urls:\n",
    "        url = urls.split(', ')[0]\n",
    "        if url in urlcache:\n",
    "            if url not in info_cache:\n",
    "                page_text = urlcache[url]\n",
    "                info = singer_band_info(url, page_text)\n",
    "                info_cache[url] = info\n",
    "                #print(\"%s Born: %s Years Active: %s\" % (url, info['born'], info['ya']))\n",
    "                ind = flatframe.index[flatframe['url'].apply(lambda s: s.split(', ')[0].find(url) == 0)].tolist()\n",
    "                flatframe.loc[ind, ['born', 'ya']] = [info['born'], info['ya']]\n",
    "        else:\n",
    "            print(\"No such url!: %s\" % url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatframe.sort_values(by='ranking').head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 What is the age at which singers achieve their top ranking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram of the age at which singers achieve their top ranking. What conclusions can you draw from this distribution of ages?\n",
    "\n",
    "*HINT: You will need to do some manipulation of the `born` column, and find the song for which a band or an artist achieves their top ranking. You will then need to put these rows together into another dataframe or array to make the plot.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ff_w_born = flatframe.copy()\n",
    "ff_w_born = ff_w_born[ff_w_born['born'] != False]\n",
    "# Get the birth year to calculate age\n",
    "ff_w_born['byear'] = ff_w_born['born'].apply(lambda s: int(s.split('-')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singer_w_born = ff_w_born.groupby('first_singer')\n",
    "singer_w_born_dict = dict()\n",
    "\n",
    "for name, group in singer_w_born:\n",
    "    top_rank = group['ranking'].agg(np.min)\n",
    "    top_group = group[group['ranking'] == top_rank]\n",
    "    top_year = top_group['year'].agg(np.max)\n",
    "    byear = top_group['byear'].tolist()[0] # Index doesn't matter because they are all the same\n",
    "    top_age = top_year - byear\n",
    "    singer_w_born_dict[name] = top_age\n",
    "\n",
    "singer_top_frame = pd.DataFrame.from_dict(singer_w_born_dict, orient='index')\n",
    "singer_top_frame.columns = ['top_age']\n",
    "singer_top_frame = singer_top_frame.sort_values(by='top_age')\n",
    "singer_top_frame.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_defaults()\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "top_age = singer_top_frame['top_age']\n",
    "singer_top_frame.plot.hist(bins=24)\n",
    "\n",
    "ax = plt.gca()\n",
    "plt.title(\"Age at which singer reach top ranking\");\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Age\")\n",
    "xt = np.arange(10, 60, 5)\n",
    "plt.xticks(xt)\n",
    "ax.legend_.remove()\n",
    "\n",
    "age_percent = [top_age.quantile(.1), top_age.quantile(.25), top_age.mean(), top_age.quantile(.75), top_age.quantile(.9)]\n",
    "for a in age_percent:\n",
    "    plt.axvline(a, 0, .9, color='r', ls='--');\n",
    "    plt.annotate('%.1f' % a, xy=(a-1, 62), va='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Most of the singers reach their personal top ranking at the age between 20 and 38 (80%), with 50% of them are within the age of 24 to 32 (50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 At what year since inception do bands reach their top rankings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a similar calculation to plot a histogram of the years since inception at which bands reach their top ranking. What conclusions can you draw?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bandframe = flatframe.copy()\n",
    "bandframe = bandframe[(bandframe['born'] == False) & (bandframe['ya'] != False)]\n",
    "# Get the year of band formation\n",
    "bandframe['inception'] = bandframe['ya'].apply(lambda s: int(s.split('â€“')[0][:4]))\n",
    "band_w_ya = bandframe.groupby('first_singer')\n",
    "band_dict = dict()\n",
    "\n",
    "for name, group in band_w_ya:\n",
    "    top_rank = group['ranking'].agg(np.min)\n",
    "    top_group = group[group['ranking'] == top_rank]\n",
    "    top_year = top_group['year'].agg(np.max)\n",
    "    inception = top_group['inception'].tolist()[0] # Index doesn't matter because they are all the same\n",
    "    top_age = top_year - inception\n",
    "    band_dict[name] = top_age\n",
    "\n",
    "band_top_frame = pd.DataFrame.from_dict(band_dict, orient='index')\n",
    "band_top_frame.columns = ['top_age']\n",
    "band_top_frame = band_top_frame.sort_values(by='top_age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_defaults()\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "top_age = band_top_frame['top_age']\n",
    "num = np.arange(len(band_top_frame))\n",
    "band_top_frame.plot.hist(bins=24)\n",
    "\n",
    "ax = plt.gca()\n",
    "plt.title(\"Years since formation to reach top ranking\");\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Years\")\n",
    "xt = np.arange(0, 50, 5)\n",
    "plt.xticks(xt)\n",
    "ax.legend_.remove()\n",
    "\n",
    "age_percent = [top_age.quantile(.5), top_age.quantile(.8)]\n",
    "for a in age_percent:\n",
    "    plt.axvline(a, 0, .9, color='r', ls='--');\n",
    "    plt.annotate('%.1f' % a, xy=(a-1, 78), va='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Most of the bands reach their top ranking within 10 years of their formation (80%). 50% of the bands achieve this in 5 years."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
